## Contributor
|         Name          | Student ID |
|-----------------------|------------|
| Chay Wen Ning         | 1201103431 |

# K-Means Clustering Lab Enhancement

This lab enhancement provides a comprehensive exploration of K-Means clustering and its variants. The enhancements include a clarification on the importance of feature scaling, a comparison between different K-Means variants, and a real-world application of K-Means in image compression.

## Enhancements

### 1. Importance of Feature Scaling

Feature scaling is a crucial preprocessing step in many machine learning algorithms, including K-Means. This section clarifies why feature scaling is important and demonstrates its impact on the performance and accuracy of the K-Means algorithm.

**Key Points:**
- Ensures all features contribute equally to the distance calculations.
- Improves the convergence speed of the K-Means algorithm.
- Demonstrated using examples with and without feature scaling.

### 2. Comparison Between K-Means Variants

This section provides a detailed comparison between standard K-Means, Mini Batch K-Means, and K-Medoids. It includes performance analysis and suitability of each variant based on different types of datasets.

**Key Points:**
- **Standard K-Means:** Efficient, suitable for small to moderate datasets with spherical clusters.
- **Mini Batch K-Means:** Faster on large datasets, approximates the results of standard K-Means.
- **K-Medoids:** More robust to outliers, suitable for small datasets with arbitrary-shaped clusters.

### 3. Real-World Application: Image Compression

K-Means clustering can be used for image compression by reducing the number of distinct colors in an image. This section demonstrates how to apply K-Means for this purpose and provides a comparison of results for different level of compression based on number of clusters.

